{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "# NLTK for pre-processing\n",
    "import nltk\n",
    "\n",
    "\n",
    "import string\n",
    "# Spacy for pre-processing\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.feature_extraction._stop_words import ENGLISH_STOP_WORDS\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import  train_test_split\n",
    "\n",
    "SOURCE_FILE_PATH = \"reviews.csv\"\n",
    "TRAIN_FILE_PATH = \"train.csv\"\n",
    "VALID_FILE_PATH = \"valid.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(SOURCE_FILE_PATH, sep='\\t')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment classify \n",
    "+ 0 = negative (ratings 1 & 2)\n",
    "+ 1 = neutral (rating 3)\n",
    "+ 2 = positive (ratings 4 & 5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_to_sentiment(rating_value):\n",
    "    if rating_value >= 4:\n",
    "        return 2\n",
    "    if rating_value == 3:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "df['Sentiment'] = df['RatingValue'].apply(map_to_sentiment)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(df['Sentiment'], return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Number'] = df.index + 1\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['Number','Sentiment', 'Review']]\n",
    "df.set_index('Number', inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')\n",
    "# from nltk.corpus import stopwords\n",
    "\n",
    "# nlp = spacy.load('en_core_web_sm')\n",
    "# punctuations = string.punctuation\n",
    "\n",
    "# STOPLIST = set(stopwords.words('english') + list(ENGLISH_STOP_WORDS))\n",
    "# SYMBOLS = string.punctuation\n",
    "\n",
    "# def data_cleaning(doc):\n",
    "#     doc = nlp(doc, disable=['parser', 'ner'])\n",
    "#     tokens = [str(token).lower() for token in doc]\n",
    "#     tokens = [token for token in tokens if token not in STOPLIST and token not in SYMBOLS]\n",
    "#     return ' '.join(tokens)\n",
    "\n",
    "# df['Review'] = df['Review'].apply(data_cleaning)\n",
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the total number of each kind of reviews\n",
    "negative_num = df[df['Sentiment'] == 0].shape[0]  \n",
    "neutral_num = df[df['Sentiment'] == 1].shape[0]   \n",
    "positive_num = df[df['Sentiment'] == 2].shape[0] \n",
    "\n",
    "print(f\"Negative review count: {negative_num}\")\n",
    "print(f\"Neutral review count: {neutral_num}\")\n",
    "print(f\"Positive review count: {positive_num}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to balance the numbers from each class with 300 observations \n",
    "positive_samples = df[df['Sentiment'] == 2].sample(n = 300, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_samples = df[df['Sentiment'] == 1].sample(n=300, random_state=42, replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_copies_needed = 300 // negative_num\n",
    "remainder = 300 % negative_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_reviews = df[df['Sentiment'] == 0]\n",
    "negative_samples =pd.concat([negative_reviews] * review_copies_needed + [negative_reviews.tail(remainder)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "balanced_data = pd.concat([negative_samples, neutral_samples, positive_samples])\n",
    "print(balanced_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### split the data into training and valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = train_test_split(balanced_data, test_size=0.3, random_state=42)\n",
    "train.to_csv(TRAIN_FILE_PATH)\n",
    "valid.to_csv(VALID_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(TRAIN_FILE_PATH)\n",
    "X_train = train['Review']\n",
    "y_train = train['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([('tokenizer', CountVectorizer()),\n",
    "                 ('tdidftransformer', TfidfTransformer()),\n",
    "                 ('categicalmodel', LogisticRegression())])\n",
    "pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(filename):\n",
    "\n",
    "    new_df = pd.read_csv(filename)\n",
    "    X_new = new_df['Review']\n",
    "    y_new = new_df['Sentiment']\n",
    "    \n",
    "    y_pred = pipe.predict(X_new)\n",
    "    print('Accuracy:', metrics.accuracy_score(y_new, y_pred))\n",
    "    print('Average F1 Score:', metrics.f1_score(y_new, y_pred, average='macro'))\n",
    "    f1_scores = metrics.f1_score(y_new, y_pred, average=None)\n",
    "    print('Class-wise F1 scores:')\n",
    "    print(f'  negative: {f1_scores[0]:.3f}')\n",
    "    print(f'   neutral: {f1_scores[1]:.3f}')\n",
    "    print(f'    positive: {f1_scores[2]:.3f}')\n",
    "    confusion_matrix = metrics.confusion_matrix(y_new, y_pred)\n",
    "    print('Confusion_matrix:')\n",
    "    print('            negative neutral positive')\n",
    "    print(f'negative     {confusion_matrix[0][0]}        {confusion_matrix[0][1]}       {confusion_matrix[0][2]}')\n",
    "    print(f'neutral      {confusion_matrix[1][0]}        {confusion_matrix[1][1]}       {confusion_matrix[1][2]}')\n",
    "    print(f'positive     {confusion_matrix[2][0]}        {confusion_matrix[2][1]}       {confusion_matrix[2][2]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(VALID_FILE_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
